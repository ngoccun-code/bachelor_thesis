% !TeX spellcheck = en_US
\chapter{Conclusion}  \label{chap:seven}%%

In conclusion, our thesis, conducted as part of the AUTOtech.agil project, has been dedicated to enhancing the 2D detector within the Providentia Mono3D object perception pipeline, with a focus on leveraging instance segmentation models to improve the system's final 3D perception performance, particularly on the TUM Traffic Intersection dataset.

Initially, we extend 1238 frames of the TUMTraf Intersection dataset with both modal and amodal instance masks to address the challenge of lacking segmentation annotations. Additionally, we present a simple approach for 2D annotation interpolation, utilizing algorithms like Jonker-Volgenant and linear interpolation to speed up the annotation process by at least five times.

Furthermore, our exploration into the state-of-the-art YOLOv8 segmentation model has yielded promising results. We extensively examine the effectiveness of pre-training on various datasets including COCO, KINS, and nuImages as well as fine-tuning on the annotated frames of the TUMTraf Intersection Dataset. Our experiments demonstrate the superiority of YOLOv8x pre-trained on COCO over the baseline YOLOv7 used in the Providentia live system, showing improvements of 3.6\% in 2D mAP@[.5:.95] and 1.53\% in 3D mAP@10. The fine-tuning of YOLOv8x on the TUMTraf Intersection dataset further improves performance, achieving a 10.5\% improvement in 2D mAP@[.5:.95] and 5.88\% in 3D mAP@10 compared to the baseline. Notably, this model excels in detecting large objects, addressing a significant limitation of the baseline YOLOv7 detector. The most significant performance boost, however, is achieved by the YOLOv8x model trained solely on the TUMTraf Intersection Dataset, with an 18.30\% improvement in 2D mAP@[.5:.95] and 7.53\% in 3D mAP@10 compared to the baseline. Together with the use of time-shifted ground truth labels and Poly-MOT 3D tracker, the best-trained model achieves a 3D mAP@10 of 34.02\% on the test sequence of the TUMTraf Intersection dataset, which is a 17.79\% improvement compared to the baseline YOLOv7 model trained on COCO. 

However, since all annotated frames come from the TUMTraf Intersection Dataset only, the trained models are overfitting on this intersection setting. This has been proven in our experiments on a highway sequence. 

In terms of speed, YOLOv8x exhibits accelerated inference speed. Once exported to TensorRT, this model achieves an inference speed of 200 FPS at a 640x640 resolution and 28 FPS at a 1920x1920 resolution, which is a 2.3 times speedup compared to the YOLOv7 model. This advancement allows for inference on full-resolution frames of 1920 while maintaining the system's real-time characteristics. The inference on full-resolution frames of 1920 has been shown to have 16.5\% higher 2D mAP@[.5:.95] than inference at 640 resolution. This highlights the significance of resolution in object detection accuracy, especially for small and far away object detection.

Additionally, we investigate the performance implications of segmenting full object masks instead of only visible ones using the amodal segmentation model C2F. Surprisingly, our experimental results indicate that utilizing amodal masks leads to a lower final 3D perception performance on the TUMTraf Intersection test set compared to solely relying on visible object masks detected by YOLOv8x. 


