\chapter{Background} \label{chap:two}%

This chapter provides essential background information. Firstly, \Cref{sec:ros} introduces the Robot Operating System 1 (ROS1), a middlewares utilized in the Providentia Mono3D system. Following this, \Cref{section:segmentation_dataset} offers a comprehensive overview of common instance segmentation datasets for autonomous driving, including a detailed description of our TUM Traffic Intersection Dataset (\Cref{section:TUMTrafIntersectionDataset}). Additionally, \Cref{sec:label format} introduces some popular annotation formats in the field of object detection, describing their labeling structures and conventions. This section is followed by an exploration of the Computer Vision Annotation Tool (CVAT) in \Cref{sec:cvat}, which plays an important role in annotation during our research.

\section{Robot Operating System 1 (ROS1)} \label{sec:ros}

The Robot Operating System, abbreviated as ROS, is not an actual operating system but a collection of software libraries and tools that support the development and reuse of code across different robotic applications. Moreover, ROS's usefulness extends beyond robots, as most of the provided tools focus on working with peripheral hardware. ROS is open-source and maintained by many people. There are two versions, ROS1 and ROS2. Since our system exploits ROS1, in this section, we introduce the concept of ROS1. 

The way ROS1 operates is relatively straightforward. A ROS system consists of several independent processes called \textit{ROS nodes}. ROS nodes can be located on different systems and can even have different architectures, making ROS genuinely flexible and adaptable to user needs. Communication is established between nodes by \textit{ROS Master}. The \textit{ROS Master} provides naming and registration services to the nodes, allowing them to find and communicate with each other.

There are two main ways of communication in ROS. The first way is communication through \textit{topic}. \textit{Nodes} can \textit{publish} or \textit{subscribe} to a \textit{topic}, then send or receive ROS messages of the topic type. This is called a publisher and subscriber relationship. The second way is communication through \textit{Service}. \textit{Nodes} provides requestable \textit{services}, other nodes can then send a \textit{request} to the service to receive a \textit{response}. The structure and type of messages between nodes are defined individually for each \textit{topic} and \textit{service}.  

\section{Autonomous Driving Segmentation Datasets} \label{section:segmentation_dataset}
Autonomous driving segmentation datasets play a crucial role in advancing computer vision algorithms in this domain. Segmentation datasets can be exploited for various tasks, including object detection, instance segmentation, and semantic segmentation. This section provides an extensive description of the commonly used instance segmentation datasets in the autonomous driving domain, as illustrated in \Cref{tab:segmentation_datasets}. Specifically, the three datasets COCO, KINS, and nuImages utilized in this thesis are examined in more detail. Afterward, the TUM Traffic Intersection Dataset, which is utilized to fine-tune the models, is described. 

\begin{table}[htb]
	\centering
	\scriptsize
	\begin{tabular}{p{1.3cm}p{5cm}p{4.5cm}p{2.6cm}}
		\hline
		\textbf{Name} & \textbf{Classes} & \textbf{Annotations} & \textbf{Data Split}\\
		\hline \raggedright
		COCO & 91 object classes with 80 used for instance segmentation & 2d bounding box, instance- and panoptic- segmentation, captioning, keypoint, dense pose & Train: 118k, Val: 5k, Test: 41k \\
		\hline 
		COCOA & same as COCO & COCO annotations plus semantic amodal segmentation & Train: 2476, Test: 1223 \\
		\hline
		KINS & 8 classes: pedestrian, cyclist, person-siting, car, tram, truck, van, misc & amodal instance mask, semantic label, and relative occlusion order & Train: 7474, Test: 7517 \\
		\hline
		nuImages & 23 classes. The supercategory Vehicle contains: Bus, Car, Construction, Emergency, Motorcycle, Trailer, Truck & 2d bounding box, instance mask & Train: 67k, Val: 16k, Test: 10k \\
		\hline
		Cityscapes \cite{cityscapes} & 30 classes grouped into eight categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void) & semantic, instance-wise, and dense pixel annotation & Train: 2975, Val: 500, Test: 1525 \\
		\hline
		Waymo Open Perception \cite{waymoOpenDataset} & 4 object classes: Vehicles, Pedestrians, Cyclists, Signs & 2D and 3D bounding boxes, key point, 3D semantic segmentation, 2D video panoptic segmentation & 390k frames, Train and Val: 1000 scenes, Test: 150 scenes \\
		\hline
	\end{tabular}
	\caption{This table provides an overview of common autonomous driving segmentation datasets including the dataset name, the object categories defined, the annotation types, and the number of samples contained in each data split.}
	\label{tab:segmentation_datasets}
\end{table}

\subsection{COCO}

Microsoft Common Objects in Context (COCO) \cite{lin2014microsoft} is a large-scale dataset for image classification, object detection, semantic segmentation, and instance segmentation. The dataset comprises images taken from daily life scenes with varying resolutions. It contains 2.5 million labeled instances in 328k images, covering 91 object classes, of which 80 are used for instance segmentation. Based on the 2017 COCO split, the data split is 118k training images, 5k validation images, and 41k testing images. Along with bounding boxes and per-instance segmentation masks, this dataset also provides natural language descriptions (captioning), key points, stuff image segmentation, panoptic segmentation, and dense pose annotation.

This dataset is extended to an amodal dataset in 2017 in \cite{cocoa}. The extended dataset is named COCOA and includes 2476 images in the training set and 1223 images in the testing set, with additional semantic amodal segmentation annotations. 

\subsection{KINS}

KITTI INStance segmentation dataset (KINS) \cite{kins} is a large-scale street scene amodal instance dataset, which is built upon Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI)  \cite{kitti}. The dataset consists of 7474 training images and 7517 testing images with eight categories: pedestrian, cyclist, person-siting, car, tram, truck, van, and misc (ambiguous vehicles). The annotations include amodal instance masks, semantic labels, and relative occlusion orders, from which initial instance masks can be easily inferred. On average, each image has 12.53 labeled instances, and each object polygon consists of 33.70 points. 53.6\% are partially occluded, and the average occlusion ratio is 31.7\%. The annotation format follows the COCO style, which will be described in \Cref{sec:cocoformat}

\subsection{nuImages} 

Following the success of the nuScenes \cite{nuscenes} dataset, in August 2020, Motional released nuImages \cite{nuImages} with additional 2D annotations from a much larger pool of data. From a total of 1.2 million autonomous driving camera images, active learning techniques were employed to select approximately 75\% challenging images according to the uncertainty of an image-based object detector. Rare classes, like bicycles, were given special focus. The remaining 25\% of the images were uniformly sampled to ensure a representative dataset and avoid strong bias. After careful review, some images were discarded due to camera artifacts, because they were too dark, or because they showed pedestrians' faces. This careful curation of a dataset resulted in a diverse dataset regarding class distribution, spatiotemporal distribution, and weather and lighting conditions. The annotated images encompass rain, snow, and nighttime, which are crucial for autonomous driving applications. 

The final dataset contains 93k labeled images with instance masks and 2D boxes, which results in around 800k foreground objects and 100k semantic segmentation masks. Additionally, each annotated image is accompanied by six past and six future unlabeled camera images at 2 Hz, leading to 93k video clips with 13 frames spaced out at 2 Hz. 

\subsection{TUMTraf Intersection Dataset} \label{section:TUMTrafIntersectionDataset}

The TUM Traffic Intersection Dataset, previously known as the "A9 Intersection Dataset" \cite{zimmer2023a9}, was first published in June 2023. This dataset is the second release (R2) of the A9 dataset \cite{a9dataset}, and it comprises 4.8k synchronized images and LiDAR point clouds with over 57.4k manually labeled 3D bounding boxes. The dataset was captured using two roadside cameras and two LiDAR mounted on intersecting gantry bridges. The data labels follow the OpenLABEL format \cite{openlabel}, which is discussed in detail in \Cref{section:openlabel}.

This dataset consists of four continuous camera and labeled LiDAR scenes captured at a frame rate of 10 Hz. Each scene is recorded from two camera perspectives, denoted as south1 and south2, yielding a total of eight frame sequences. Scenes S1 and S2 each consist of 600 frames, depicting a daytime scenario at dusk. Scene S3 consists of 2400 frames captured during daytime with sunshine. Scene S4 contains an additional 1200 frames recorded at night and during heavy rain \cite{a9dataset}.

It differentiates ten categories: CAR, BUS, TRUCK, TRAILER, VAN, MOTORCYCLE, BICYCLE, PEDESTRIAN, EMERGENCY\_VEHICLE, and OTHER. The dominant category is CAR, followed by TRUCKS, TRAILER, VAN, and PEDESTRIAN by approximately the same order of magnitude. The remaining five classes are present in slightly smaller numbers. Of all the objects in the dataset, 78.2\% were classified as NOT OCCLUDED, 16.1\% as PARTIALLY OCCLUDED, 0.8\% as MOSTLY OCCLUDED, and 4.9\% UNKNOWN. 

\section{Label Format} \label{sec:label format}

Each model and each dataset has its own specific requirements regarding data structure and label format. Understanding these aspects is critical for training deep learning models and managing datasets. This section introduces three well-known annotation formats used in this work, including OpenLABEL, YOLO, and COCO Format.

\subsection{OpenLABEL Format} \label{section:openlabel}

The Association for Standardization of Automation and Measuring Systems (ASAM) with Deepen AI released the OpenLABEL \cite{openlabel}, a standard designed to specify an annotation format that is flexible enough to support the development of automated driving features while ensuring interoperability among different systems and providers. The annotation structure of OpenLABEL is defined using JSON schema, and annotation files are stored in .json format.

The TUMTraf dataset has fully adopted this annotation format, with each frame having one .json label file. The general structure of one point cloud 3D label file is shown in Listing 2.1. Each object has one unique object identifier. The object data is a collection of "name," "type," which defines the category of the object, and "cuboid," which describes the 3D bounding box. 

\begin{lstlisting}[language=json, caption={Illutration of OpenLABEL Annotation JSON Structure}, label=lst:openlabel_structure]
	{"openlabel:" {
			"metadata": {<...>},
			"coordinate_system": [<...>],
			"frames": {
				"<frame_id>" : {
					"frame_properties" : {<...>},
					"objects" : {
						"<object1_id>" : {
							"object_data": {
								"name" : <str>, 
								"type" : <str>, 
								"cuboid" : {
									"name": "shape3D",
									"val": <[...]>,
									"attributes": <{...}>
								}
							}
						},
						"<object2_id>" : {<...>}
						"<object3_id>" : {<...>}
						<...>
					}
				}
			}
		}
	}
\end{lstlisting}

\subsection{YOLO Format} 

The YOLO label format is tailored for YOLO (You Only Look Once) models. For each frame, there is one annotation text file (.txt). Each row in the text file corresponds to one object instance in the image. The format for one object bounding box is
\begin{center}
	\code{<class-index>  <x\_center>  <y\_center>  <width>  <height> } 
\end{center}
whereas for the bounding coordinates of the object's segmentation mask is
\begin{center}
	\code{<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>} 
\end{center}
The bounding box coordinates and segmentation mask points are normalized to the range [0, 1]. While YOLO's compact label format simplifies annotation storage, users must ensure consistency and distinguish between bounding box and mask annotations.

\subsection{COCO Format} \label{sec:cocoformat}

Common Objects in Context (COCO) is a well-known dataset format used by Microsoft, Google, and Facebook  \cite{cocoformat}. This format also exploits the JSON structure. In this format, the labels of all objects from all frames will all be included in one .json file, a collection of “info,” “licenses,” “images,” “annotations,” and “categories”. 

\begin{lstlisting}[language=json, keepspaces=true]
	{
		"info"        : <info>, 
		"images"      : <[image]>, 
		"annotations" : <[annotation]>, 
		"licenses"    : <[license]>,
	}
\end{lstlisting}

The “annotations” section contains a list of every individual object annotation. The structure of one object detection annotation is shown in Listing 2.2. Each annotation includes information such as the category label, bounding box coordinates, segmentation mask, and additional metadata. The segmentation mask can be represented either as a run-length-encoded bit mask or as a list of polygon contour points, providing flexibility in representing object boundaries. The 'iscrowd' field specifies whether the segmentation is for a single object or for a cluster of objects, while the 'area' field indicates the area of the object mask, measured in pixels.

\begin{lstlisting}[language=json, caption={Illutration of COCO Annotation JSON Structure}, keepspaces=true, label=lst:coco_structure] 
	annotation  {
		"id"          : <int>, 
		"image_id"    : <int>, 
		"category_id" : <int>, 
		"segmentation": <RLE or [polygon]>, 
		"area"        : <float>, 
		"bbox"        : <[x_top_left,y_top_left,width,height]>, 
		"iscrowd"     : <int> 
	}
\end{lstlisting}

%\section{Labeling Tools}  \label{sec:labeling_tools}
% Various annotation tools are available to assist in the labeling of images for computer vision tasks. Here, we compare <amount> labeling tools: Computer Vision Annotation Tool (CVAT), ..., ...
%\hl{Compare different tools.}
%\subsection{Computer Vision Annotation Tool (CVAT)}  \label{sec:cvat}
\section{Computer Vision Annotation Tool (CVAT)}  \label{sec:cvat}

Computer Vision Annotation Tool \cite{cvat} is a web-based, open-source annotation tool initially developed by Intel and now maintained by OpenCV. CVAT allows users to annotate images with various types of shapes, including boxes, polygons, polylines, and points. It supports importing and exporting annotations in multiple formats, such as YOLO, MS COCO, and KITTI. One of its standout features is its ability to perform automatic labeling using serverless deep learning models like YOLO, RCNN, Face Detection, and  Segment Anything. This feature can significantly speed up the labeling process for large datasets.

