% !TeX spellcheck = en_US
\chapter{Outlook and Future Work}  \label{chap:eight}%%

In this chapter, we outline potential future work to address identified improvements from our implementation and evaluation process: 

\begin{itemize}
	\item As mentioned in \Cref{sec:2d_interpolation_pipeline}, the proposed 2D annotation interpolation pipeline utilizing the Jonker-Volgenant algorithm still sometimes generates erroneous object matching, requiring manual refinement afterward. Moreover, the currently used linear interpolation assumes a constant velocity of road users between frames. Future work could explore more advanced data association algorithms and leverage the existing Poly-MOT tracker to interpolate the 2D masks to more precise locations.
	
	\item Currently, only a limited number of frames, over one thousand, from the TUM Traffic Intersection dataset are annotated. While effective for training models specific to this intersection, this limited dataset poses a risk of overfitting. Consequently, models trained on this data exhibit improved performance at the intersection but suffer decreased performance on highway scenes compared to the baseline model YOLOv7. To address this limitation and train more generalized models, it is essential to annotate the remaining frames of the TUM Traffic Intersection Dataset and other releases of the TUM Traffic Dataset, such as the R00 and R01 release containing highway scenes and extended highway scenes in adverse weather conditions. The proposed 2D annotation interpolation pipeline can be refined and utilized for this task. 
	
	\item Another potential approach is to train separate models specifically tailored for intersection and highway scenes. By doing so, each model can be optimized for its respective environment, potentially leading to improved performance in both settings. Particular attention should be paid to overfitting prevention. The model can overfit quickly on the small amount of annotated frames. To prevent this, closely monitor the validation performance.
	
	\item The YOLOv8 model pre-trained on nuImages for 50 epochs is still not powerful enough to have any improvement over the model pre-trained on COCO. Given the vastness of the nuImages dataset and its potential benefits for model performance, our ongoing efforts involve continuing to pre-train the YOLOv8x segmentation model on nuImages. We are aiming for 200 epochs, and when the performance improves compared to pre-train on COCO, we will also fine-tune the model on the TUM Traffic Intersection dataset. 
	
	\item The trained YOLOv8 segmentation models can be exploited to run 2D multiple-object tracking. Ultralytics has implemented track algorithms, including BoT-SORT and ByteTrack, for the YOLOv8 model. The tracker produces the same output as segmentation, with an additional object ID that remains consistent for each object across consecutive frames. Therefore, after training a good and generalized model, future work can exploit 2D multiple-object tracking and investigate the performance of the two supported track algorithms. Exploiting a 2D multiple-object tracker with YOLOv8 is expected to enhance the overall performance by providing more stable predictions.
	
	\item As discussed in \Cref{sec:object_detection_with_yolov8}, transmitting segmentation instance masks in the form of polygon contours may offer greater efficiency compared to using bit-packed masks. Therefore, future work could adjust the 3D detector's message reception to accept polygon contours, eliminating the need to transmit bit-packed masks altogether.
	
	\item As revealed in \Cref{sec:quan_3d}, the 3D bounding box labels of the TUMTraf Intersection Dataset are incomplete, resulting in false positives during model evaluation. Future work should involve revising these labels, particularly for categories such as PEDESTRIAN, MOTORCYCLE, and BICYCLE, to ensure more accurate model evaluation and performance assessment.
	
	\item A YOLOv8 segmentation model trained on full-resolution frames can detect big objects successfully, however, with multiple overlapping masks. Future work can implement post-processing, which does not filter out the overlapping masks but merges them to receive one final mask with good coverage over the detected object. Objects from the frames taken at the intersection do not overlap much with each other. Therefore, an IoU threshold of around 0.6 to 0.7 can be used to decide if two masks are referring to the same object and should be merged. 
	
	\item Current evaluations of the C2F amodal segmentation model do not show improvement over only segmenting the visible part. The current C2F architecture crops frames based on each object's region of interest (ROI), resizes them to 256x256, extends them to amodal masks, and rescales them back to their original size within the original frame. However, as the system currently operates on the full resolution of 1920, 256x256 is relatively small, even for an object in the frame. This means that after upscaling a predicted amodal mask to its original size, the mask contour can be very unsmooth, which can affect the following bottom contour extraction step of the 2D-to-3D lifting stage of the system. In future work, the architecture of C2F can be adjusted to scale the ROIs to a bigger size.
	
\end{itemize}