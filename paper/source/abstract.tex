% In total max. 1 Page!
\AIRstudentthesisAbstract{%
	%
	% Abstract English:
	As part of the AUTOtech.agil project, this thesis aims to enhance the 2D detector node within the Providentia Mono3D system by leveraging instance segmentation models, thereby improving the system's capacity to predict real-world 3D object locations and shapes. Addressing the absence of segmentation labels in the TUM Traffic Dataset, we extend a subset with both modal and amodal segmentation annotations. To achieve this, we propose a 2D annotation interpolation pipeline capable of interpolating annotations between consecutive frames. We employ the YOLOv8x segmentation model and extensively investigate the effectiveness of pre-training on various datasets including COCO, KINS, and nuImages, along with training on the annotated frames. The best-performing model achieves a 2D mAP@[.5:.95] of 75.90 and a 3D mAP@[.10] of 18.51, which is an 18.30\% and 7.53\% improvement over the current implementation utilizing YOLOv7. Additionally, we employ the amodal segmentation model C2F to extend to amodal detections and evaluate its impact on final 3D perception performance, which, contrary to expectations, exhibits a decline. Further experiments are conducted across nighttime and highway scenarios, accompanied by suggestions for future improvements such as extending the training dataset on different scenarios for better model generalization. 
	% The code will be released at https://github.com/zhang-tao-whu/e2ec.
}{%
	%
	% Zusammenfassung Deutsch:
	
	Im Rahmen des AUTOtech.agil-Projekts zielt diese Arbeit darauf ab, den 2D Detektor des Providentia Mono3D-Systems durch die Verwendung von Instanzsegmentierungsmodellen zu verbessern, um die 3D-Wahrnehmungsfähigkeit des realen Systems zu verbessern. Um dem Fehlen an Segmentierungsannotationen im TUM Traffic Intersection Datensatz entgegenzuwirken, erweitern wir eine Teilsatz um modale und amodale Segmentierungsmasken. Hierfür schlagen wir eine 2D-Annotationsinterpolationspipeline vor, die in der Lage ist, Annotationen zwischen aufeinanderfolgenden Frames zu interpolieren. Wir verwenden das YOLOv8x Instanzsegmentierungsmodell und untersuchen die Wirksamkeit von Vortrainings auf verschiedenen Datensätzen, darunter COCO, KINS und nuImages, sowie das Training auf annotierten Frames. Das beste Modell erreicht eine 2D mAP@[.5:.95] von 75.90 und eine 3D mAP@[.10] von 18.51, eine Verbesserung von 18,30\% und 7,53\% gegenüber der aktuellen Implementierung mit YOLOv7. Darüber hinaus verwenden wir das amodale Segmentierungsmodell C2F, um die sichtbaren Detektionen auf amodale Detektionen auszudehnen, und evaluieren dessen Auswirkungen auf die endgültige 3D- Wahrnehmungsleistung, die entgegen den Erwartungen einen Rückgang zeigt. Weitere Experimente werden in Nacht- und Autobahnszenarien durchgeführt, begleitet von Vorschlägen für zukünftige Verbesserungen, wie die Erweiterung des Trainingsdatensatzes auf verschiedene Szenarien für eine bessere Modellverallgemeinerung.
	
	%
}%
%
%
